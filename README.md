# hag-bigquery-terraform

Terraform-scripts for 친 opprette BigQuery-datasett og tabeller for HAG.

Basert p친 [bomlo-bigquery-terraform](https://github.com/navikt/bomlo-bigquery-terraform) og flex sitt repo flex-bigquery-terraform

## Hvordan kj칮re Terraform lokalt

Dersom terraform allerede er satt opp og initiert for repo slik at bygg p친 GA fungerer, og du kun 칮nsker 친 inspisere hva terraform finner og rapporterer av endringer:

F칮lg oppskriften under fra og med pkt. 4 (Installer Terraform lokalt) til pkt. 6 (Kj칮r kode lokalt).

## Hvordan sette opp repo med Terraform f칮rste gang
Opprettelse av bucket og bruk av denne for terraform state m친 gj칮res i to separate steg. Dette m친 gj칮res lokalt fordi uten terraform state i bucketen s친 vil ikke GitHub Actions ha mulighet til 친 ta vare p친 state mellom kj칮ringer. Hvis vi pr칮ver 친 gj칮re dette via GitHub Actions vil bruk av bucket for terraform state feile da den i tillegg vil fors칮ke 친 opprette bucketen p친 nytt, fordi staten ikke har spor av opprettelsen av bucketen.
1. Opprett service account i GCP med permissions:
    * BigQuery Data Owner
    * Editor
    * Secret Manager Secret Accessor

   I dette repoet er det opprettet en bruker med navn `terraform` i `tbd-dev` og `tbd-prod` med disse tilgangene.

2. Opprett key for service account for hhv [dev](https://console.cloud.google.com/iam-admin/serviceaccounts?project=tbd-dev-7ff9) og [prod](https://console.cloud.google.com/iam-admin/serviceaccounts?project=tbd-prod-eacd)

    * Velg "Manage keys" fra actions for terraform-kontoen
    * Velg "Add keys" -> "Create new key" -> "Key type=JSON" -> "Create"
    * Key lagres til fil lokalt
    * Flytt filene til hjemmeomr친de og endre rettigheter slik: chmod go-rwx ~/tbd-terraform-*.key

3. Legg inn filen med service account keyen i [GitHub secret](https://github.com/navikt/bomlo-bigquery-terraform/settings/secrets/actions) (v친re secrets heter GCP_SECRET_DEV og GCP_SECRET_PROD)

4. Installer Terraform lokalt

   F.eks med brew: `brew install terraform`

5. Velg milj칮 og logg inn
    - G친 til mappe du skal kj칮re terraform fra (prod eller dev): `cd dev`
    - Sett context (dev-gcp/prod-gcp): `kubectl config use-context dev-gcp`
    - Kj칮r kommando: `gcloud auth application-default login`

6. Kj칮r kode lokalt for 친 opprette bucket (men ikke pr칮v 친 bruke den enda). Se kode i [commit](https://github.com/navikt/bomlo-bigquery-terraform/commit/3a6b7edb78a29052cd1e1dfae54c5ac3404768f8)
    ```
    terraform init
    terraform plan -refresh-only -detailed-exitcode
    ``` 
7. Gj칮r eventuelle endringer i terraform-filer, og for 친 se resultatet av dem kj칮r f칮lgende kommando:
   ```
   terraform plan -detailed-exitcode
   ```
   (forskjellen p친 `terraform plan -refresh-only` og `terraform plan` kan du lese om [her](https://medium.com/code-oil/understanding-terraform-plan-apply-refresh-only-the-myths-and-fixing-drift-5963207a1df8))
8. N친r du er forn칮yd med endringene terraform rapporterer om i punktet over, kj칮r f칮lgende kommando:
    ```
    terraform apply
    ```  
9. Kj칮r kode lokalt for 친 bruke bucket for state. Se kode i [commit](https://github.com/navikt/bomlo-bigquery-terraform/commit/42b61393184652e12f2efaf9bb974e7c7cfbeefb)
     ```
    terraform init
    ```   
10. Endre context til milj칮 det ikke er kj칮rt for 친 gjenta n칮dvendige steg over.
11. N친 kan workflowen pushes


## Hvordan sette opp en datastream i GCP med terraform

Legg merke til bruken av denne sv칝rt interessante emojien游녢

游볞: betyr at dette steget kan gjenbrukes for flere datastreams og er allerede p친 plass for `tbd-dev` og `tbd-prod`. Dvs. er du en b칮mlis s친 kan du mest sannynlig hoppe over dette steget!


### Forutsetninger
Databasen man 칮nsker 친 streame til Bigquery m친 v칝re klargjort. Dette inneb칝rer:
1. Enable logical decoding, se [her](https://github.com/navikt/helse-dataprodukter/blob/5041c1cfd9fb85fb48ea0de2e3ac3882b4e3d0b6/arbeidsgiveropplysninger/deploy/nais.yml#L37)
2. Lag en databasebruker, se [her](https://github.com/navikt/helse-dataprodukter/blob/5041c1cfd9fb85fb48ea0de2e3ac3882b4e3d0b6/arbeidsgiveropplysninger/deploy/nais.yml#L35)
3. Gi den nye brukeren og den generelle databasebrukeren riktige tilganger, se [migrering V3](https://github.com/navikt/helse-dataprodukter/blob/main/forstegangsbehandling/src/main/resources/db/migration/V3__datastream_grants.sql)
    * NB: burde gj칮res i en commit etter punktet over for 친 unng친 race condition
4. Opprett publication og replication slots, se [migrering V4](https://github.com/navikt/helse-dataprodukter/blob/main/forstegangsbehandling/src/main/resources/db/migration/V4__datastream_publication.sql)
   og [V5](https://github.com/navikt/helse-dataprodukter/blob/main/forstegangsbehandling/src/main/resources/db/migration/V5__datastream_replication.sql)

Hvis du f친r problemer med 친 kj칮re testene s친 trengs det muligens noen endringer i testconfigen. Pr칮v 친 legge til
`"-c", "wal_level=logical"` i PostgreSQLContaineren, se [her](https://github.com/navikt/helse-dataprodukter/blob/3e4245321e3ba5bf8e221b7e7ee8581d864c9d27/arbeidsgiveropplysninger/src/test/kotlin/arbeidsgiveropplysninger/TestDatabase.kt#L18)


### Steg for 친 sette opp datastream

1. 游볞 Lag en VPC (Virtual Private Cloud) (f.eks. `tbd_datastream_private_vpc`)
2. 游볞 Lag en IP-range (f.eks. `tbd_datastream_vpc_ip_range`)
3. Gi databasen en private IP manuelt i GCP. NB. databasen f친r nedetid i dette steget 游땸 (f.eks. `dataprodukt-arbeidsgiveropplysninger`)
    1. G친 til databasen i GCP
    2. Trykk _Edit_
    3. Trykk p친 _Connections_
    4. Huk av for _Private IP_
    5. Velg VPC-en du lagde i punkt 1.
    6. Trykk _Set up connection_ (kun f칮rste gang per prosjekt)
    7. Trykk _Enable API_ (kun f칮rste gang per prosjekt)
    8. Velg IP-range du lagde i punkt 2. (kun f칮rste gang per prosjekt)
    9. Trykk p친 _Create Connection_
    10. Trykk p친 _Save_

4. 游볞 Lag datastream private connection med vpc peering med subnet (f.eks. `tbd_datastream_private_connection`)
5. Oppsett av firewallregler og reverse proxy, gj칮r en av f칮lgende punkter:
    * Hvis du har satt opp dette fra f칮r m친 du legge til:
        1. Den nye databasen som proxy instance, se [her](https://github.com/navikt/bomlo-bigquery-terraform/commit/08af6cda5adfc8ee07e0d13c7a61bcfa7cdcea0f) (se bort fra det ekstra mellomrommet som snek seg inn (og ble fjernet i neste commit))
        2. Ny firewall-regel som tillater connections fra databaseporten, se [her](https://github.com/navikt/bomlo-bigquery-terraform/blob/1349486438d25d890ef5a6a2a8603e1511db5377/prod/datastream-vpc.tf#L41)
    * 游볞 Hvis du ikke har satt opp firewall regler eller laget reverse proxy m친 dette gj칮res slik som [her](https://github.com/navikt/bomlo-bigquery-terraform/commit/08f5d25cd1956cd686874247b51608031c979f85)

   Etter 친 ha gjort dette m친 du resette proxyen, se [Stuck](#stuck)

6. Lag en secret i Secret Manager manuelt i GCP for brukeren du opprettet i [Forutsetninger](#Forutsetninger):
    1. Hent ut brukerens passord og brukernavn fra secrets i kubernetes, dette opprettet nais automatisk da brukeren ble opprettet i `nais.yml`:
   ```
   brew install jq
   kubectl -n tbd get secret <navnet p친 secret> -o json | jq ".data | map_values(@base64d)"
   ```

   游눠 Usikker p친 hva secreten din heter? Du kan liste opp secrets ved 친 kj칮re kommandoen under og begynne 친 lete 游댍 Ofte starter secreten med `google`, har appnavnet i seg og slutter med en hash.
    ```
    kubectl -n tbd get secrets | grep <app-navn>
    ```
    2. G친 til Secret Manager i GCP, opprett secret, skriv f칮lgende json:
   ```
   {
        "username": "<brukernavn fra secret>",
        "password": "<passord fra secret>"
   }
   ```
    3. Lagre
7. Opprett to connection profiles, se [commit](https://github.com/navikt/bomlo-bigquery-terraform/commit/6af1542dce45ac541a670e1f07bcd3a25e98f13d):
    1. mellom database og datastream (endringene i `datastream-dataprodukt-arbeidsgiveropplysninger.tf` og `secrets.tf` i commiten)
    2. 游볞 mellom datastream og bigquery (endringene i `datastream-vpc.tf` i commiten)
8. Lag datastream (f.eks. `arbeidsgiveropplysninger_datastream`)

### Nullstille/wipe data
Dersom man i sjeldne tilfeller 칮nsker 친 nullstille og starte synkronisering av data p친 nytt er det erfaringsmessig* best 친 slette datastream og opprette den p친 nytt.
Fremgangsm친ten blir da:
1. Slett datastream (fra GCP-console)
2. Slett tilh칮rende tabeller i BigQuery (fra GCP-console)
3. Truncate tabeller i Postgres
4. Kj칮r terraform-bygget p친 nytt slik at datastream gjenopprettes

\* Backfill i kombinasjon med truncate p친 kildetabellene har vist seg 친 v칝re litt tricky. Det kan virke som at datastreamen holder p친 tidligere data som har blitt truncated.
### Stuck
* N친r du legger til nye proxy instances s친 er det behov for 친 resette VM-en (den finner du p친 GCP: Compute Engine 俱뫮잺 VM instances 俱뫮잺 trykk p친 din VM 俱뫮잺 trykk p친 reset)
